{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f93511f4-d721-48e9-8f6a-d01be90bf521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'LLaMA-Factory'...\n",
      "remote: Enumerating objects: 356, done.\u001b[K\n",
      "remote: Counting objects: 100% (356/356), done.\u001b[K\n",
      "remote: Compressing objects: 100% (275/275), done.\u001b[K\n",
      "remote: Total 356 (delta 76), reused 267 (delta 66), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (356/356), 9.72 MiB | 5.83 MiB/s, done.\n",
      "Resolving deltas: 100% (76/76), done.\n",
      "Updating files: 100% (289/289), done.\n",
      "/mnt/c/work/alma/hrChatBot/LLaMA-Factory\n",
      "\u001b[0m\u001b[01;32mCITATION.cff\u001b[0m*  \u001b[01;32mREADME.md\u001b[0m*     \u001b[34;42mdocker\u001b[0m/          \u001b[01;32mrequirements.txt\u001b[0m*  \u001b[34;42mtests\u001b[0m/\n",
      "\u001b[01;32mLICENSE\u001b[0m*       \u001b[01;32mREADME_zh.md\u001b[0m*  \u001b[34;42mevaluation\u001b[0m/      \u001b[34;42mscripts\u001b[0m/\n",
      "\u001b[01;32mMANIFEST.in\u001b[0m*   \u001b[34;42massets\u001b[0m/        \u001b[34;42mexamples\u001b[0m/        \u001b[01;32msetup.py\u001b[0m*\n",
      "\u001b[01;32mMakefile\u001b[0m*      \u001b[34;42mdata\u001b[0m/          \u001b[01;32mpyproject.toml\u001b[0m*  \u001b[34;42msrc\u001b[0m/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///mnt/c/work/alma/hrChatBot/LLaMA-Factory\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting transformers!=4.46.*,!=4.47.*,!=4.48.0,!=4.52.0,<=4.52.1,>=4.45.0 (from llamafactory==0.9.3.dev0)\n",
      "  Downloading transformers-4.52.1-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting datasets<=3.6.0,>=2.16.0 (from llamafactory==0.9.3.dev0)\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting accelerate<=1.7.0,>=0.34.0 (from llamafactory==0.9.3.dev0)\n",
      "  Downloading accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting peft<=0.15.2,>=0.14.0 (from llamafactory==0.9.3.dev0)\n",
      "  Downloading peft-0.15.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting trl<=0.9.6,>=0.8.6 (from llamafactory==0.9.3.dev0)\n",
      "  Using cached trl-0.9.6-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting tokenizers<=0.21.1,>=0.19.0 (from llamafactory==0.9.3.dev0)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: gradio<=5.30.0,>=4.38.0 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from llamafactory==0.9.3.dev0) (5.29.0)\n",
      "Collecting scipy (from llamafactory==0.9.3.dev0)\n",
      "  Downloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting einops (from llamafactory==0.9.3.dev0)\n",
      "  Using cached einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting sentencepiece (from llamafactory==0.9.3.dev0)\n",
      "  Using cached sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting tiktoken (from llamafactory==0.9.3.dev0)\n",
      "  Downloading tiktoken-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting protobuf (from llamafactory==0.9.3.dev0)\n",
      "  Downloading protobuf-6.31.0-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: uvicorn in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from llamafactory==0.9.3.dev0) (0.34.2)\n",
      "Requirement already satisfied: fastapi in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from llamafactory==0.9.3.dev0) (0.115.12)\n",
      "Collecting sse-starlette (from llamafactory==0.9.3.dev0)\n",
      "  Downloading sse_starlette-2.3.5-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting matplotlib>=3.7.0 (from llamafactory==0.9.3.dev0)\n",
      "  Downloading matplotlib-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting fire (from llamafactory==0.9.3.dev0)\n",
      "  Using cached fire-0.7.0-py3-none-any.whl\n",
      "Collecting omegaconf (from llamafactory==0.9.3.dev0)\n",
      "  Using cached omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: packaging in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from llamafactory==0.9.3.dev0) (24.2)\n",
      "Requirement already satisfied: pyyaml in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from llamafactory==0.9.3.dev0) (6.0.2)\n",
      "Collecting numpy<2.0.0 (from llamafactory==0.9.3.dev0)\n",
      "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting pydantic<=2.10.6 (from llamafactory==0.9.3.dev0)\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from llamafactory==0.9.3.dev0) (2.2.3)\n",
      "Collecting av (from llamafactory==0.9.3.dev0)\n",
      "  Downloading av-14.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
      "Collecting librosa (from llamafactory==0.9.3.dev0)\n",
      "  Downloading librosa-0.11.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting tyro<0.9.0 (from llamafactory==0.9.3.dev0)\n",
      "  Using cached tyro-0.8.14-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting torch>=2.0.0 (from llamafactory==0.9.3.dev0)\n",
      "  Downloading torch-2.7.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Collecting torchvision>=0.15.0 (from llamafactory==0.9.3.dev0)\n",
      "  Downloading torchvision-0.22.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting bitsandbytes>=0.39.0 (from llamafactory==0.9.3.dev0)\n",
      "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: psutil in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from accelerate<=1.7.0,>=0.34.0->llamafactory==0.9.3.dev0) (7.0.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from accelerate<=1.7.0,>=0.34.0->llamafactory==0.9.3.dev0) (0.31.1)\n",
      "Collecting safetensors>=0.4.3 (from accelerate<=1.7.0,>=0.34.0->llamafactory==0.9.3.dev0)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: filelock in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.18.0)\n",
      "Collecting pyarrow>=15.0.0 (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading pyarrow-20.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: requests>=2.32.2 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (4.67.1)\n",
      "Collecting xxhash (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0)\n",
      "  Using cached xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0)\n",
      "  Using cached multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.11.18)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from gradio<=5.30.0,>=4.38.0->llamafactory==0.9.3.dev0) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from gradio<=5.30.0,>=4.38.0->llamafactory==0.9.3.dev0) (4.9.0)\n",
      "Requirement already satisfied: ffmpy in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from gradio<=5.30.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.5.0)\n",
      "Requirement already satisfied: gradio-client==1.10.0 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from gradio<=5.30.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.10.0)\n",
      "Requirement already satisfied: groovy~=0.1 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from gradio<=5.30.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.1.2)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from gradio<=5.30.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.28.1)\n",
      "Requirement already satisfied: jinja2<4.0 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from gradio<=5.30.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from gradio<=5.30.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.0.2)\n",
      "Requirement already satisfied: orjson~=3.0 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from gradio<=5.30.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.10.18)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from gradio<=5.30.0,>=4.38.0->llamafactory==0.9.3.dev0) (11.2.1)\n",
      "Requirement already satisfied: pydub in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from gradio<=5.30.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from gradio<=5.30.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.0.20)\n",
      "Requirement already satisfied: ruff>=0.9.3 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from gradio<=5.30.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.11.9)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from gradio<=5.30.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from gradio<=5.30.0,>=4.38.0->llamafactory==0.9.3.dev0) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from gradio<=5.30.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.46.2)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from gradio<=5.30.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.13.2)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from gradio<=5.30.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.15.3)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from gradio<=5.30.0,>=4.38.0->llamafactory==0.9.3.dev0) (4.13.2)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from gradio-client==1.10.0->gradio<=5.30.0,>=4.38.0->llamafactory==0.9.3.dev0) (15.0.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio<=5.30.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio<=5.30.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio<=5.30.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from pandas>=2.0.0->llamafactory==0.9.3.dev0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from pandas>=2.0.0->llamafactory==0.9.3.dev0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from pandas>=2.0.0->llamafactory==0.9.3.dev0) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from pydantic<=2.10.6->llamafactory==0.9.3.dev0) (0.7.0)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic<=2.10.6->llamafactory==0.9.3.dev0)\n",
      "  Using cached pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate<=1.7.0,>=0.34.0->llamafactory==0.9.3.dev0) (1.1.0)\n",
      "Collecting regex!=2019.12.17 (from transformers!=4.46.*,!=4.47.*,!=4.48.0,!=4.52.0,<=4.52.1,>=4.45.0->llamafactory==0.9.3.dev0)\n",
      "  Using cached regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio<=5.30.0,>=4.38.0->llamafactory==0.9.3.dev0) (8.2.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio<=5.30.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio<=5.30.0,>=4.38.0->llamafactory==0.9.3.dev0) (14.0.0)\n",
      "Collecting docstring-parser>=0.16 (from tyro<0.9.0->llamafactory==0.9.3.dev0)\n",
      "  Using cached docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting shtab>=1.5.6 (from tyro<0.9.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (1.20.0)\n",
      "Collecting sympy>=1.13.3 (from torch>=2.0.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=2.0.0->llamafactory==0.9.3.dev0)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch>=2.0.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch>=2.0.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch>=2.0.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch>=2.0.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch>=2.0.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch>=2.0.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.7.77 (from torch>=2.0.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch>=2.0.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch>=2.0.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch>=2.0.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.26.2 (from torch>=2.0.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.6.77 (from torch>=2.0.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch>=2.0.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch>=2.0.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.3.0 (from torch>=2.0.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading triton-3.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from triton==3.3.0->torch>=2.0.0->llamafactory==0.9.3.dev0) (57.4.0)\n",
      "Requirement already satisfied: certifi in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from httpx>=0.24.1->gradio<=5.30.0,>=4.38.0->llamafactory==0.9.3.dev0) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from httpx>=0.24.1->gradio<=5.30.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio<=5.30.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.16.0)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0)\n",
      "  Using cached contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading fonttools-4.58.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (104 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0)\n",
      "  Using cached kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.2 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0)\n",
      "  Using cached pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: six>=1.5 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->llamafactory==0.9.3.dev0) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from requests>=2.32.2->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from requests>=2.32.2->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (2.4.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio<=5.30.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio<=5.30.0,>=4.38.0->llamafactory==0.9.3.dev0) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio<=5.30.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.1.2)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=2.0.0->llamafactory==0.9.3.dev0)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting termcolor (from fire->llamafactory==0.9.3.dev0)\n",
      "  Downloading termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting audioread>=2.1.9 (from librosa->llamafactory==0.9.3.dev0)\n",
      "  Using cached audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting numba>=0.51.0 (from librosa->llamafactory==0.9.3.dev0)\n",
      "  Downloading numba-0.61.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
      "Collecting scikit-learn>=1.1.0 (from librosa->llamafactory==0.9.3.dev0)\n",
      "  Using cached scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting joblib>=1.0 (from librosa->llamafactory==0.9.3.dev0)\n",
      "  Downloading joblib-1.5.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from librosa->llamafactory==0.9.3.dev0) (5.2.1)\n",
      "Collecting soundfile>=0.12.1 (from librosa->llamafactory==0.9.3.dev0)\n",
      "  Using cached soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl.metadata (16 kB)\n",
      "Collecting pooch>=1.1 (from librosa->llamafactory==0.9.3.dev0)\n",
      "  Using cached pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting soxr>=0.3.2 (from librosa->llamafactory==0.9.3.dev0)\n",
      "  Using cached soxr-0.5.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting lazy_loader>=0.1 (from librosa->llamafactory==0.9.3.dev0)\n",
      "  Using cached lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting msgpack>=1.0 (from librosa->llamafactory==0.9.3.dev0)\n",
      "  Using cached msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba>=0.51.0->librosa->llamafactory==0.9.3.dev0)\n",
      "  Using cached llvmlite-0.44.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from pooch>=1.1->librosa->llamafactory==0.9.3.dev0) (4.3.8)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn>=1.1.0->librosa->llamafactory==0.9.3.dev0)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: cffi>=1.0 in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from soundfile>=0.12.1->librosa->llamafactory==0.9.3.dev0) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->llamafactory==0.9.3.dev0) (2.22)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from omegaconf->llamafactory==0.9.3.dev0)\n",
      "  Using cached antlr4_python3_runtime-4.9.3-py3-none-any.whl\n",
      "Downloading accelerate-1.7.0-py3-none-any.whl (362 kB)\n",
      "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Using cached multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Downloading peft-0.15.2-py3-none-any.whl (411 kB)\n",
      "Downloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Using cached pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Downloading transformers-4.52.1-py3-none-any.whl (10.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached trl-0.9.6-py3-none-any.whl (245 kB)\n",
      "Using cached tyro-0.8.14-py3-none-any.whl (109 kB)\n",
      "Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m95.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.7.0-cp310-cp310-manylinux_2_28_x86_64.whl (865.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.2/865.2 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Downloading triton-3.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.4/156.4 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading matplotlib-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.58.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "Downloading pyarrow-20.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (42.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hUsing cached pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Using cached regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Downloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Downloading torchvision-0.22.0-cp310-cp310-manylinux_2_28_x86_64.whl (7.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading av-14.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.8/34.8 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Downloading librosa-0.11.0-py3-none-any.whl (260 kB)\n",
      "Using cached audioread-3.0.1-py3-none-any.whl (23 kB)\n",
      "Downloading joblib-1.5.0-py3-none-any.whl (307 kB)\n",
      "Using cached lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Using cached msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (378 kB)\n",
      "Downloading numba-0.61.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached llvmlite-0.44.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n",
      "Using cached pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "Using cached scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "Downloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hUsing cached soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl (1.3 MB)\n",
      "Using cached soxr-0.5.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (252 kB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "Downloading protobuf-6.31.0-cp39-abi3-manylinux2014_x86_64.whl (320 kB)\n",
      "Using cached sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Downloading sse_starlette-2.3.5-py3-none-any.whl (10 kB)\n",
      "Downloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading tiktoken-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Building wheels for collected packages: llamafactory\n",
      "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llamafactory: filename=llamafactory-0.9.3.dev0-0.editable-py3-none-any.whl size=27248 sha256=32ecae86e47490b915d76152d18e054ff3bb561f567bf90364935926539aaf6f\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-b55u9y8c/wheels/61/cc/54/8dc8388dfee5010fc8dd0c730dd82aa127c583570b90c1ae38\n",
      "Successfully built llamafactory\n",
      "Installing collected packages: sentencepiece, nvidia-cusparselt-cu12, mpmath, antlr4-python3-runtime, xxhash, triton, threadpoolctl, termcolor, sympy, shtab, safetensors, regex, pyparsing, pydantic-core, pyarrow, protobuf, omegaconf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, msgpack, llvmlite, lazy_loader, kiwisolver, joblib, fsspec, fonttools, einops, docstring-parser, dill, cycler, av, audioread, tiktoken, soxr, soundfile, scipy, pydantic, pooch, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, numba, multiprocess, fire, contourpy, tyro, tokenizers, scikit-learn, nvidia-cusolver-cu12, matplotlib, transformers, torch, sse-starlette, librosa, torchvision, datasets, bitsandbytes, accelerate, trl, peft, llamafactory\n",
      "\u001b[2K  Attempting uninstall: pydantic-core━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/70\u001b[0m [pyparsing]s]l]-runtime]\n",
      "\u001b[2K    Found existing installation: pydantic_core 2.33.2━━━━━━━━━\u001b[0m \u001b[32m12/70\u001b[0m [pyparsing]\n",
      "\u001b[2K    Uninstalling pydantic_core-2.33.2:━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/70\u001b[0m [pyparsing]\n",
      "\u001b[2K      Successfully uninstalled pydantic_core-2.33.2━━━━━━━━━━━\u001b[0m \u001b[32m12/70\u001b[0m [pyparsing]\n",
      "\u001b[2K  Attempting uninstall: numpy0m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25/70\u001b[0m [nvidia-cublas-cu12]u12]2]\n",
      "\u001b[2K    Found existing installation: numpy 2.2.5━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25/70\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K    Uninstalling numpy-2.2.5:1m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26/70\u001b[0m [numpy]las-cu12]\n",
      "\u001b[2K      Successfully uninstalled numpy-2.2.5━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26/70\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: fsspec\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32/70\u001b[0m [joblib]ader]\n",
      "\u001b[2K    Found existing installation: fsspec 2025.3.2━━━━━━━━━━━━━━\u001b[0m \u001b[32m32/70\u001b[0m [joblib]\n",
      "\u001b[2K    Uninstalling fsspec-2025.3.2:1m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33/70\u001b[0m [fsspec]\n",
      "\u001b[2K      Successfully uninstalled fsspec-2025.3.2━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33/70\u001b[0m [fsspec]\n",
      "\u001b[2K  Attempting uninstall: pydantic━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m44/70\u001b[0m [scipy]ile]parser]\n",
      "\u001b[2K    Found existing installation: pydantic 2.11.4━━━━━━━━━━━━━━\u001b[0m \u001b[32m44/70\u001b[0m [scipy]\n",
      "\u001b[2K    Uninstalling pydantic-2.11.4:\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m45/70\u001b[0m [pydantic]\n",
      "\u001b[2K      Successfully uninstalled pydantic-2.11.4m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m45/70\u001b[0m [pydantic]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70/70\u001b[0m [llamafactory]afactory]]erate]s]]r-cu12]\n",
      "\u001b[1A\u001b[2KSuccessfully installed accelerate-1.7.0 antlr4-python3-runtime-4.9.3 audioread-3.0.1 av-14.4.0 bitsandbytes-0.45.5 contourpy-1.3.2 cycler-0.12.1 datasets-3.6.0 dill-0.3.8 docstring-parser-0.16 einops-0.8.1 fire-0.7.0 fonttools-4.58.0 fsspec-2025.3.0 joblib-1.5.0 kiwisolver-1.4.8 lazy_loader-0.4 librosa-0.11.0 llamafactory-0.9.3.dev0 llvmlite-0.44.0 matplotlib-3.10.3 mpmath-1.3.0 msgpack-1.1.0 multiprocess-0.70.16 networkx-3.4.2 numba-0.61.2 numpy-1.26.4 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 omegaconf-2.3.0 peft-0.15.2 pooch-1.8.2 protobuf-6.31.0 pyarrow-20.0.0 pydantic-2.10.6 pydantic-core-2.27.2 pyparsing-3.2.3 regex-2024.11.6 safetensors-0.5.3 scikit-learn-1.6.1 scipy-1.15.3 sentencepiece-0.2.0 shtab-1.7.2 soundfile-0.13.1 soxr-0.5.0.post1 sse-starlette-2.3.5 sympy-1.14.0 termcolor-3.1.0 threadpoolctl-3.6.0 tiktoken-0.9.0 tokenizers-0.21.1 torch-2.7.0 torchvision-0.22.0 transformers-4.52.1 triton-3.3.0 trl-0.9.6 tyro-0.8.14 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
    "%cd LLaMA-Factory\n",
    "%ls\n",
    "!pip install -e .[torch,bitsandbytes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58d2f828-0730-4918-9fc5-fcebefa3524f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May 23 12:02:28 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.133.07             Driver Version: 560.94         CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        On  |   00000000:02:00.0  On |                  N/A |\n",
      "|  0%  ERR!    P8             19W /  350W |    1424MiB /  24576MiB |      1%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A             381      G   /Xwayland                             N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f42727d-5c25-4a01-8596-d6967ffa82c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/work/alma/hrChatBot/LLaMA-Factory\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd776263-766a-45fa-9cb2-eab08455d90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visit http://ip:port for Web UI, e.g., http://127.0.0.1:7860\n",
      "* Running on local URL:  http://0.0.0.0:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n",
      "/usr/bin/xdg-open: 882: x-www-browser: not found\n",
      "/usr/bin/xdg-open: 882: firefox: not found\n",
      "/usr/bin/xdg-open: 882: iceweasel: not found\n",
      "/usr/bin/xdg-open: 882: seamonkey: not found\n",
      "/usr/bin/xdg-open: 882: mozilla: not found\n",
      "/usr/bin/xdg-open: 882: epiphany: not found\n",
      "/usr/bin/xdg-open: 882: konqueror: not found\n",
      "/usr/bin/xdg-open: 882: chromium: not found\n",
      "/usr/bin/xdg-open: 882: chromium-browser: not found\n",
      "/usr/bin/xdg-open: 882: google-chrome: not found\n",
      "/usr/bin/xdg-open: 882: www-browser: not found\n",
      "/usr/bin/xdg-open: 882: links2: not found\n",
      "/usr/bin/xdg-open: 882: elinks: not found\n",
      "/usr/bin/xdg-open: 882: links: not found\n",
      "/usr/bin/xdg-open: 882: lynx: not found\n",
      "/usr/bin/xdg-open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'http://localhost:7860/'\n",
      "[INFO|2025-05-23 12:17:09] llamafactory.hparams.parser:401 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: torch.bfloat16\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-05-23 12:17:09,945 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-05-23 12:17:09,946 >> loading file tokenizer.model from cache at None\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-05-23 12:17:09,946 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-05-23 12:17:09,946 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-05-23 12:17:09,946 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-05-23 12:17:09,946 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|tokenization_utils_base.py:2299] 2025-05-23 12:17:10,367 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:698] 2025-05-23 12:17:11,801 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json\n",
      "[INFO|configuration_utils.py:770] 2025-05-23 12:17:11,870 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-05-23 12:17:12,546 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-05-23 12:17:12,547 >> loading file tokenizer.model from cache at None\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-05-23 12:17:12,547 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-05-23 12:17:12,547 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-05-23 12:17:12,547 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-05-23 12:17:12,547 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|tokenization_utils_base.py:2299] 2025-05-23 12:17:12,873 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|2025-05-23 12:17:12] llamafactory.data.template:143 >> Add pad token: <|eot_id|>\n",
      "[INFO|2025-05-23 12:17:12] llamafactory.data.template:143 >> Add <|eom_id|> to stop words.\n",
      "[INFO|2025-05-23 12:17:12] llamafactory.data.loader:143 >> Loading dataset /mnt/c/work/alma/hrChatBot/hr_bot.json...\n",
      "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 1549 examples [00:00, 9691.15 examples/s]\n",
      "Converting format of dataset (num_proc=16): 100%|█| 1549/1549 [00:00<00:00, 5479\n",
      "Running tokenizer on dataset (num_proc=16): 100%|█| 1549/1549 [00:03<00:00, 439.\n",
      "training example:\n",
      "input_ids:\n",
      "[128000, 128006, 882, 128007, 271, 3923, 374, 279, 8712, 315, 3857, 220, 17, 13, 16, 30, 128009, 128006, 78191, 128007, 271, 6919, 5659, 5492, 320, 33231, 39, 8, 11216, 128009]\n",
      "inputs:\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is the topic of section 2.1?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Work From Home (WFH) Policy<|eot_id|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 6919, 5659, 5492, 320, 33231, 39, 8, 11216, 128009]\n",
      "labels:\n",
      "Work From Home (WFH) Policy<|eot_id|>\n",
      "[INFO|configuration_utils.py:698] 2025-05-23 12:17:18,897 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json\n",
      "[INFO|configuration_utils.py:770] 2025-05-23 12:17:18,898 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|2025-05-23 12:17:18] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n",
      "[INFO|modeling_utils.py:1149] 2025-05-23 12:17:28,531 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:2239] 2025-05-23 12:17:28,533 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1135] 2025-05-23 12:17:28,535 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:08<00:00,  4.08s/it]\n",
      "[INFO|modeling_utils.py:5170] 2025-05-23 12:17:36,807 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:5178] 2025-05-23 12:17:36,807 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-3.2-3B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:1090] 2025-05-23 12:17:37,078 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/generation_config.json\n",
      "[INFO|configuration_utils.py:1135] 2025-05-23 12:17:37,079 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "[INFO|2025-05-23 12:17:37] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n",
      "[INFO|2025-05-23 12:17:37] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
      "[INFO|2025-05-23 12:17:37] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\n",
      "[INFO|2025-05-23 12:17:37] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA\n",
      "[INFO|2025-05-23 12:17:37] llamafactory.model.model_utils.misc:143 >> Found linear modules: k_proj,o_proj,gate_proj,down_proj,q_proj,v_proj,up_proj\n",
      "[INFO|2025-05-23 12:17:38] llamafactory.model.loader:143 >> trainable params: 12,156,928 || all params: 3,224,906,752 || trainable%: 0.3770\n",
      "[INFO|trainer.py:756] 2025-05-23 12:17:38,956 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2409] 2025-05-23 12:17:39,239 >> ***** Running training *****\n",
      "[INFO|trainer.py:2410] 2025-05-23 12:17:39,239 >>   Num examples = 1,549\n",
      "[INFO|trainer.py:2411] 2025-05-23 12:17:39,240 >>   Num Epochs = 20\n",
      "[INFO|trainer.py:2412] 2025-05-23 12:17:39,240 >>   Instantaneous batch size per device = 4\n",
      "[INFO|trainer.py:2415] 2025-05-23 12:17:39,240 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:2416] 2025-05-23 12:17:39,240 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:2417] 2025-05-23 12:17:39,240 >>   Total optimization steps = 980\n",
      "[INFO|trainer.py:2418] 2025-05-23 12:17:39,244 >>   Number of trainable parameters = 12,156,928\n",
      "  2%|▊                                         | 20/980 [01:07<53:51,  3.37s/it][INFO|2025-05-23 12:18:47] llamafactory.train.callbacks:143 >> {'loss': 2.0795, 'learning_rate': 4.7500e-06, 'epoch': 0.41, 'throughput': 892.89}\n",
      "{'loss': 2.0795, 'grad_norm': 1.135292649269104, 'learning_rate': 4.75e-06, 'epoch': 0.41, 'num_input_tokens_seen': 60544, 'train_runtime': 67.8114, 'train_tokens_per_second': 892.829}\n",
      "  4%|█▋                                        | 40/980 [02:08<46:39,  2.98s/it][INFO|2025-05-23 12:19:48] llamafactory.train.callbacks:143 >> {'loss': 2.0389, 'learning_rate': 9.7500e-06, 'epoch': 0.82, 'throughput': 931.55}\n",
      "{'loss': 2.0389, 'grad_norm': 1.184898018836975, 'learning_rate': 9.750000000000002e-06, 'epoch': 0.82, 'num_input_tokens_seen': 120128, 'train_runtime': 128.96, 'train_tokens_per_second': 931.513}\n",
      "  6%|██▌                                       | 60/980 [03:10<45:54,  2.99s/it][INFO|2025-05-23 12:20:49] llamafactory.train.callbacks:143 >> {'loss': 1.9584, 'learning_rate': 1.4750e-05, 'epoch': 1.23, 'throughput': 939.09}\n",
      "{'loss': 1.9584, 'grad_norm': 1.6163870096206665, 'learning_rate': 1.475e-05, 'epoch': 1.23, 'num_input_tokens_seen': 178880, 'train_runtime': 190.4863, 'train_tokens_per_second': 939.07}\n",
      "  8%|███▍                                      | 80/980 [04:14<46:36,  3.11s/it][INFO|2025-05-23 12:21:53] llamafactory.train.callbacks:143 >> {'loss': 1.6606, 'learning_rate': 1.9750e-05, 'epoch': 1.64, 'throughput': 944.97}\n",
      "{'loss': 1.6606, 'grad_norm': 0.8445591926574707, 'learning_rate': 1.9750000000000002e-05, 'epoch': 1.64, 'num_input_tokens_seen': 240160, 'train_runtime': 254.1489, 'train_tokens_per_second': 944.958}\n",
      " 10%|████▏                                    | 100/980 [05:15<41:06,  2.80s/it][INFO|2025-05-23 12:22:54] llamafactory.train.callbacks:143 >> {'loss': 1.4501, 'learning_rate': 2.4750e-05, 'epoch': 2.04, 'throughput': 945.04}\n",
      "{'loss': 1.4501, 'grad_norm': 0.7768841981887817, 'learning_rate': 2.4750000000000002e-05, 'epoch': 2.04, 'num_input_tokens_seen': 298176, 'train_runtime': 315.5209, 'train_tokens_per_second': 945.028}\n",
      " 12%|█████                                    | 120/980 [06:17<43:27,  3.03s/it][INFO|2025-05-23 12:23:57] llamafactory.train.callbacks:143 >> {'loss': 1.3559, 'learning_rate': 2.9750e-05, 'epoch': 2.45, 'throughput': 944.99}\n",
      "{'loss': 1.3559, 'grad_norm': 0.8434891700744629, 'learning_rate': 2.975e-05, 'epoch': 2.45, 'num_input_tokens_seen': 356992, 'train_runtime': 377.7787, 'train_tokens_per_second': 944.976}\n",
      " 14%|█████▊                                   | 140/980 [07:22<43:22,  3.10s/it][INFO|2025-05-23 12:25:01] llamafactory.train.callbacks:143 >> {'loss': 1.2787, 'learning_rate': 3.4750e-05, 'epoch': 2.87, 'throughput': 937.10}\n",
      "{'loss': 1.2787, 'grad_norm': 0.962073028087616, 'learning_rate': 3.475e-05, 'epoch': 2.87, 'num_input_tokens_seen': 414848, 'train_runtime': 442.6969, 'train_tokens_per_second': 937.093}\n",
      " 16%|██████▋                                  | 160/980 [08:25<42:08,  3.08s/it][INFO|2025-05-23 12:26:04] llamafactory.train.callbacks:143 >> {'loss': 1.1742, 'learning_rate': 3.9750e-05, 'epoch': 3.27, 'throughput': 931.89}\n",
      "{'loss': 1.1742, 'grad_norm': 1.2077490091323853, 'learning_rate': 3.9750000000000004e-05, 'epoch': 3.27, 'num_input_tokens_seen': 470664, 'train_runtime': 505.0695, 'train_tokens_per_second': 931.88}\n",
      " 18%|███████▌                                 | 180/980 [09:29<43:02,  3.23s/it][INFO|2025-05-23 12:27:08] llamafactory.train.callbacks:143 >> {'loss': 1.0849, 'learning_rate': 4.4750e-05, 'epoch': 3.68, 'throughput': 931.26}\n",
      "{'loss': 1.0849, 'grad_norm': 1.2182992696762085, 'learning_rate': 4.4750000000000004e-05, 'epoch': 3.68, 'num_input_tokens_seen': 530312, 'train_runtime': 569.4613, 'train_tokens_per_second': 931.252}\n",
      " 20%|████████▎                                | 200/980 [10:31<43:00,  3.31s/it][INFO|2025-05-23 12:28:10] llamafactory.train.callbacks:143 >> {'loss': 1.0346, 'learning_rate': 4.9750e-05, 'epoch': 4.08, 'throughput': 931.65}\n",
      "{'loss': 1.0346, 'grad_norm': 1.7128891944885254, 'learning_rate': 4.975e-05, 'epoch': 4.08, 'num_input_tokens_seen': 588128, 'train_runtime': 631.2786, 'train_tokens_per_second': 931.646}\n",
      " 22%|█████████▏                               | 220/980 [11:32<38:13,  3.02s/it][INFO|2025-05-23 12:29:11] llamafactory.train.callbacks:143 >> {'loss': 0.8802, 'learning_rate': 4.9927e-05, 'epoch': 4.49, 'throughput': 936.60}\n",
      "{'loss': 0.8802, 'grad_norm': 2.065105438232422, 'learning_rate': 4.992683290694946e-05, 'epoch': 4.49, 'num_input_tokens_seen': 648704, 'train_runtime': 692.6171, 'train_tokens_per_second': 936.598}\n",
      " 24%|██████████                               | 240/980 [12:36<37:12,  3.02s/it][INFO|2025-05-23 12:30:15] llamafactory.train.callbacks:143 >> {'loss': 0.8541, 'learning_rate': 4.9692e-05, 'epoch': 4.91, 'throughput': 936.43}\n",
      "{'loss': 0.8541, 'grad_norm': 1.693598747253418, 'learning_rate': 4.9692208514878444e-05, 'epoch': 4.91, 'num_input_tokens_seen': 708320, 'train_runtime': 756.4073, 'train_tokens_per_second': 936.427}\n",
      " 27%|██████████▉                              | 260/980 [13:38<36:24,  3.03s/it][INFO|2025-05-23 12:31:17] llamafactory.train.callbacks:143 >> {'loss': 0.7298, 'learning_rate': 4.9297e-05, 'epoch': 5.31, 'throughput': 933.42}\n",
      "{'loss': 0.7298, 'grad_norm': 2.7668330669403076, 'learning_rate': 4.929744567540826e-05, 'epoch': 5.31, 'num_input_tokens_seen': 763992, 'train_runtime': 818.4895, 'train_tokens_per_second': 933.417}\n",
      " 29%|███████████▋                             | 280/980 [14:42<36:26,  3.12s/it][INFO|2025-05-23 12:32:21] llamafactory.train.callbacks:143 >> {'loss': 0.6044, 'learning_rate': 4.8745e-05, 'epoch': 5.72, 'throughput': 935.14}\n",
      "{'loss': 0.6044, 'grad_norm': 2.529163360595703, 'learning_rate': 4.874510457703583e-05, 'epoch': 5.72, 'num_input_tokens_seen': 825176, 'train_runtime': 882.4177, 'train_tokens_per_second': 935.131}\n",
      " 31%|████████████▌                            | 300/980 [15:42<33:33,  2.96s/it][INFO|2025-05-23 12:33:22] llamafactory.train.callbacks:143 >> {'loss': 0.5278, 'learning_rate': 4.8039e-05, 'epoch': 6.12, 'throughput': 935.72}\n",
      "{'loss': 0.5278, 'grad_norm': 3.13226580619812, 'learning_rate': 4.803876736373483e-05, 'epoch': 6.12, 'num_input_tokens_seen': 882368, 'train_runtime': 942.987, 'train_tokens_per_second': 935.716}\n",
      " 33%|█████████████▍                           | 320/980 [16:47<33:19,  3.03s/it][INFO|2025-05-23 12:34:26] llamafactory.train.callbacks:143 >> {'loss': 0.3857, 'learning_rate': 4.7183e-05, 'epoch': 6.54, 'throughput': 934.51}\n",
      "{'loss': 0.3857, 'grad_norm': 3.195934534072876, 'learning_rate': 4.71830149033774e-05, 'epoch': 6.54, 'num_input_tokens_seen': 941696, 'train_runtime': 1007.6897, 'train_tokens_per_second': 934.51}\n",
      " 35%|██████████████▏                          | 340/980 [17:51<33:45,  3.16s/it][INFO|2025-05-23 12:35:31] llamafactory.train.callbacks:143 >> {'loss': 0.3667, 'learning_rate': 4.6183e-05, 'epoch': 6.95, 'throughput': 935.20}\n",
      "{'loss': 0.3667, 'grad_norm': 3.8921051025390625, 'learning_rate': 4.6183397079048205e-05, 'epoch': 6.95, 'num_input_tokens_seen': 1002400, 'train_runtime': 1071.8631, 'train_tokens_per_second': 935.194}\n",
      " 37%|███████████████                          | 360/980 [18:54<34:49,  3.37s/it][INFO|2025-05-23 12:36:33] llamafactory.train.callbacks:143 >> {'loss': 0.2434, 'learning_rate': 4.5046e-05, 'epoch': 7.35, 'throughput': 935.44}\n",
      "{'loss': 0.2434, 'grad_norm': 2.8245906829833984, 'learning_rate': 4.5046396795922964e-05, 'epoch': 7.35, 'num_input_tokens_seen': 1061032, 'train_runtime': 1134.2669, 'train_tokens_per_second': 935.434}\n",
      " 39%|███████████████▉                         | 380/980 [19:57<36:29,  3.65s/it][INFO|2025-05-23 12:37:36] llamafactory.train.callbacks:143 >> {'loss': 0.2202, 'learning_rate': 4.3779e-05, 'epoch': 7.76, 'throughput': 936.23}\n",
      "{'loss': 0.2202, 'grad_norm': 3.4591317176818848, 'learning_rate': 4.377938793714064e-05, 'epoch': 7.76, 'num_input_tokens_seen': 1121256, 'train_runtime': 1197.6357, 'train_tokens_per_second': 936.225}\n",
      " 41%|████████████████▋                        | 400/980 [20:58<28:21,  2.93s/it][INFO|2025-05-23 12:38:37] llamafactory.train.callbacks:143 >> {'loss': 0.1684, 'learning_rate': 4.2391e-05, 'epoch': 8.16, 'throughput': 936.27}\n",
      "{'loss': 0.1684, 'grad_norm': 3.2465741634368896, 'learning_rate': 4.239058754134175e-05, 'epoch': 8.16, 'num_input_tokens_seen': 1178096, 'train_runtime': 1258.2909, 'train_tokens_per_second': 936.267}\n",
      " 43%|█████████████████▌                       | 420/980 [22:01<27:20,  2.93s/it][INFO|2025-05-23 12:39:40] llamafactory.train.callbacks:143 >> {'loss': 0.1161, 'learning_rate': 4.0889e-05, 'epoch': 8.58, 'throughput': 938.63}\n",
      "{'loss': 0.1161, 'grad_norm': 3.079739570617676, 'learning_rate': 4.088900251201964e-05, 'epoch': 8.58, 'num_input_tokens_seen': 1240592, 'train_runtime': 1321.7108, 'train_tokens_per_second': 938.626}\n",
      " 45%|██████████████████▍                      | 440/980 [23:05<27:05,  3.01s/it][INFO|2025-05-23 12:40:44] llamafactory.train.callbacks:143 >> {'loss': 0.1119, 'learning_rate': 3.9284e-05, 'epoch': 8.99, 'throughput': 936.93}\n",
      "{'loss': 0.1119, 'grad_norm': 2.480586528778076, 'learning_rate': 3.928437120429502e-05, 'epoch': 8.99, 'num_input_tokens_seen': 1297968, 'train_runtime': 1385.3485, 'train_tokens_per_second': 936.925}\n",
      " 47%|███████████████████▏                     | 460/980 [24:07<27:52,  3.22s/it][INFO|2025-05-23 12:41:46] llamafactory.train.callbacks:143 >> {'loss': 0.0651, 'learning_rate': 3.7587e-05, 'epoch': 9.39, 'throughput': 937.25}\n",
      "{'loss': 0.0651, 'grad_norm': 2.21425199508667, 'learning_rate': 3.7587100267946226e-05, 'epoch': 9.39, 'num_input_tokens_seen': 1356848, 'train_runtime': 1447.6935, 'train_tokens_per_second': 937.248}\n",
      " 49%|████████████████████                     | 480/980 [25:09<25:29,  3.06s/it][INFO|2025-05-23 12:42:49] llamafactory.train.callbacks:143 >> {'loss': 0.0636, 'learning_rate': 3.5808e-05, 'epoch': 9.80, 'throughput': 938.00}\n",
      "{'loss': 0.0636, 'grad_norm': 2.7943248748779297, 'learning_rate': 3.5808197156291937e-05, 'epoch': 9.8, 'num_input_tokens_seen': 1416368, 'train_runtime': 1509.9893, 'train_tokens_per_second': 937.999}\n",
      " 51%|████████████████████▉                    | 500/980 [26:11<24:17,  3.04s/it][INFO|2025-05-23 12:43:51] llamafactory.train.callbacks:143 >> {'loss': 0.0516, 'learning_rate': 3.3959e-05, 'epoch': 10.21, 'throughput': 937.42}\n",
      "{'loss': 0.0516, 'grad_norm': 3.3998701572418213, 'learning_rate': 3.39591987386325e-05, 'epoch': 10.21, 'num_input_tokens_seen': 1473504, 'train_runtime': 1571.8764, 'train_tokens_per_second': 937.417}\n",
      " 51%|████████████████████▉                    | 500/980 [26:11<24:17,  3.04s/it][INFO|trainer.py:3993] 2025-05-23 12:43:51,123 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/hr_bot/checkpoint-500\n",
      "[INFO|configuration_utils.py:698] 2025-05-23 12:43:51,705 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json\n",
      "[INFO|configuration_utils.py:770] 2025-05-23 12:43:51,706 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2356] 2025-05-23 12:43:52,123 >> chat template saved in saves/Llama-3.2-3B-Instruct/lora/hr_bot/checkpoint-500/chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2525] 2025-05-23 12:43:52,129 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/hr_bot/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2534] 2025-05-23 12:43:52,133 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/hr_bot/checkpoint-500/special_tokens_map.json\n",
      " 53%|█████████████████████▊                   | 520/980 [27:18<24:11,  3.16s/it][INFO|2025-05-23 12:44:57] llamafactory.train.callbacks:143 >> {'loss': 0.0391, 'learning_rate': 3.2052e-05, 'epoch': 10.62, 'throughput': 935.17}\n",
      "{'loss': 0.0391, 'grad_norm': 2.9754979610443115, 'learning_rate': 3.2052096479225566e-05, 'epoch': 10.62, 'num_input_tokens_seen': 1532256, 'train_runtime': 1638.4827, 'train_tokens_per_second': 935.168}\n",
      " 55%|██████████████████████▌                  | 540/980 [28:21<21:24,  2.92s/it][INFO|2025-05-23 12:46:00] llamafactory.train.callbacks:143 >> {'loss': 0.0362, 'learning_rate': 3.0099e-05, 'epoch': 11.02, 'throughput': 935.63}\n",
      "{'loss': 0.0362, 'grad_norm': 1.4884274005889893, 'learning_rate': 3.009925866803872e-05, 'epoch': 11.02, 'num_input_tokens_seen': 1591808, 'train_runtime': 1701.3339, 'train_tokens_per_second': 935.624}\n",
      " 57%|███████████████████████▍                 | 560/980 [29:25<26:18,  3.76s/it][INFO|2025-05-23 12:47:04] llamafactory.train.callbacks:143 >> {'loss': 0.0214, 'learning_rate': 2.8113e-05, 'epoch': 11.43, 'throughput': 936.15}\n",
      "{'loss': 0.0214, 'grad_norm': 1.628773808479309, 'learning_rate': 2.8113350207643057e-05, 'epoch': 11.43, 'num_input_tokens_seen': 1652736, 'train_runtime': 1765.4579, 'train_tokens_per_second': 936.151}\n",
      " 59%|████████████████████████▎                | 580/980 [30:27<20:11,  3.03s/it][INFO|2025-05-23 12:48:06] llamafactory.train.callbacks:143 >> {'loss': 0.0212, 'learning_rate': 2.6107e-05, 'epoch': 11.85, 'throughput': 936.28}\n",
      "{'loss': 0.0212, 'grad_norm': 1.546219825744629, 'learning_rate': 2.6107250476460037e-05, 'epoch': 11.85, 'num_input_tokens_seen': 1711136, 'train_runtime': 1827.5847, 'train_tokens_per_second': 936.283}\n",
      " 61%|█████████████████████████                | 600/980 [31:30<18:59,  3.00s/it][INFO|2025-05-23 12:49:09] llamafactory.train.callbacks:143 >> {'loss': 0.0186, 'learning_rate': 2.4094e-05, 'epoch': 12.25, 'throughput': 935.48}\n",
      "{'loss': 0.0186, 'grad_norm': 3.2360024452209473, 'learning_rate': 2.40939698010502e-05, 'epoch': 12.25, 'num_input_tokens_seen': 1768536, 'train_runtime': 1890.5151, 'train_tokens_per_second': 935.478}\n",
      " 63%|█████████████████████████▉               | 620/980 [32:35<18:27,  3.08s/it][INFO|2025-05-23 12:50:14] llamafactory.train.callbacks:143 >> {'loss': 0.0164, 'learning_rate': 2.2087e-05, 'epoch': 12.66, 'throughput': 935.98}\n",
      "{'loss': 0.0164, 'grad_norm': 1.4223532676696777, 'learning_rate': 2.2086565079152946e-05, 'epoch': 12.66, 'num_input_tokens_seen': 1829848, 'train_runtime': 1955.02, 'train_tokens_per_second': 935.974}\n",
      " 65%|██████████████████████████▊              | 640/980 [33:37<17:02,  3.01s/it][INFO|2025-05-23 12:51:17] llamafactory.train.callbacks:143 >> {'loss': 0.0137, 'learning_rate': 2.0098e-05, 'epoch': 13.06, 'throughput': 936.39}\n",
      "{'loss': 0.0137, 'grad_norm': 0.3344334065914154, 'learning_rate': 2.0098055100694376e-05, 'epoch': 13.06, 'num_input_tokens_seen': 1889560, 'train_runtime': 2017.9183, 'train_tokens_per_second': 936.391}\n",
      " 67%|███████████████████████████▌             | 660/980 [34:42<17:51,  3.35s/it][INFO|2025-05-23 12:52:21] llamafactory.train.callbacks:143 >> {'loss': 0.0099, 'learning_rate': 1.8141e-05, 'epoch': 13.47, 'throughput': 936.27}\n",
      "{'loss': 0.0099, 'grad_norm': 1.8488166332244873, 'learning_rate': 1.8141336115938855e-05, 'epoch': 13.47, 'num_input_tokens_seen': 1949816, 'train_runtime': 2082.5471, 'train_tokens_per_second': 936.265}\n",
      " 69%|████████████████████████████▍            | 680/980 [35:47<18:39,  3.73s/it][INFO|2025-05-23 12:53:26] llamafactory.train.callbacks:143 >> {'loss': 0.0113, 'learning_rate': 1.6229e-05, 'epoch': 13.89, 'throughput': 936.50}\n",
      "{'loss': 0.0113, 'grad_norm': 0.8421193957328796, 'learning_rate': 1.6229098198357408e-05, 'epoch': 13.89, 'num_input_tokens_seen': 2010744, 'train_runtime': 2147.0803, 'train_tokens_per_second': 936.502}\n",
      " 71%|█████████████████████████████▎           | 700/980 [36:47<13:48,  2.96s/it][INFO|2025-05-23 12:54:26] llamafactory.train.callbacks:143 >> {'loss': 0.0078, 'learning_rate': 1.4374e-05, 'epoch': 14.29, 'throughput': 935.81}\n",
      "{'loss': 0.0078, 'grad_norm': 0.7128804922103882, 'learning_rate': 1.4373742944631622e-05, 'epoch': 14.29, 'num_input_tokens_seen': 2065760, 'train_runtime': 2207.472, 'train_tokens_per_second': 935.803}\n",
      " 73%|██████████████████████████████           | 720/980 [37:51<12:50,  2.96s/it][INFO|2025-05-23 12:55:31] llamafactory.train.callbacks:143 >> {'loss': 0.0076, 'learning_rate': 1.2587e-05, 'epoch': 14.70, 'throughput': 935.87}\n",
      "{'loss': 0.0076, 'grad_norm': 0.25672847032546997, 'learning_rate': 1.2587303045540314e-05, 'epoch': 14.7, 'num_input_tokens_seen': 2126240, 'train_runtime': 2271.9358, 'train_tokens_per_second': 935.872}\n",
      " 76%|██████████████████████████████▉          | 740/980 [38:55<12:03,  3.01s/it][INFO|2025-05-23 12:56:34] llamafactory.train.callbacks:143 >> {'loss': 0.0065, 'learning_rate': 1.0881e-05, 'epoch': 15.10, 'throughput': 936.21}\n",
      "{'loss': 0.0065, 'grad_norm': 0.23116813600063324, 'learning_rate': 1.0881364249342482e-05, 'epoch': 15.1, 'num_input_tokens_seen': 2186080, 'train_runtime': 2335.046, 'train_tokens_per_second': 936.204}\n",
      " 78%|███████████████████████████████▊         | 760/980 [39:59<12:04,  3.29s/it][INFO|2025-05-23 12:57:38] llamafactory.train.callbacks:143 >> {'loss': 0.0059, 'learning_rate': 9.2670e-06, 'epoch': 15.52, 'throughput': 936.60}\n",
      "{'loss': 0.0059, 'grad_norm': 0.625318706035614, 'learning_rate': 9.266990223754069e-06, 'epoch': 15.52, 'num_input_tokens_seen': 2247296, 'train_runtime': 2399.4114, 'train_tokens_per_second': 936.603}\n",
      " 80%|████████████████████████████████▋        | 780/980 [41:03<11:54,  3.57s/it][INFO|2025-05-23 12:58:43] llamafactory.train.callbacks:143 >> {'loss': 0.0053, 'learning_rate': 7.7547e-06, 'epoch': 15.93, 'throughput': 936.00}\n",
      "{'loss': 0.0053, 'grad_norm': 0.6174648404121399, 'learning_rate': 7.75465080381731e-06, 'epoch': 15.93, 'num_input_tokens_seen': 2306144, 'train_runtime': 2463.8423, 'train_tokens_per_second': 935.995}\n",
      " 82%|█████████████████████████████████▍       | 800/980 [42:03<08:45,  2.92s/it][INFO|2025-05-23 12:59:43] llamafactory.train.callbacks:143 >> {'loss': 0.0040, 'learning_rate': 6.3542e-06, 'epoch': 16.33, 'throughput': 936.45}\n",
      "{'loss': 0.004, 'grad_norm': 0.7077266573905945, 'learning_rate': 6.354154091002892e-06, 'epoch': 16.33, 'num_input_tokens_seen': 2363520, 'train_runtime': 2523.9073, 'train_tokens_per_second': 936.453}\n",
      " 84%|██████████████████████████████████▎      | 820/980 [43:08<07:55,  2.97s/it][INFO|2025-05-23 13:00:47] llamafactory.train.callbacks:143 >> {'loss': 0.0045, 'learning_rate': 5.0746e-06, 'epoch': 16.74, 'throughput': 935.93}\n",
      "{'loss': 0.0045, 'grad_norm': 0.18277540802955627, 'learning_rate': 5.074582843908241e-06, 'epoch': 16.74, 'num_input_tokens_seen': 2422496, 'train_runtime': 2588.344, 'train_tokens_per_second': 935.925}\n",
      " 86%|███████████████████████████████████▏     | 840/980 [44:11<07:12,  3.09s/it][INFO|2025-05-23 13:01:50] llamafactory.train.callbacks:143 >> {'loss': 0.0038, 'learning_rate': 3.9242e-06, 'epoch': 17.14, 'throughput': 934.51}\n",
      "{'loss': 0.0038, 'grad_norm': 0.1206706315279007, 'learning_rate': 3.924235573082644e-06, 'epoch': 17.14, 'num_input_tokens_seen': 2478024, 'train_runtime': 2651.6999, 'train_tokens_per_second': 934.504}\n",
      " 88%|███████████████████████████████████▉     | 860/980 [45:15<06:27,  3.23s/it][INFO|2025-05-23 13:02:54] llamafactory.train.callbacks:143 >> {'loss': 0.0036, 'learning_rate': 2.9106e-06, 'epoch': 17.56, 'throughput': 933.51}\n",
      "{'loss': 0.0036, 'grad_norm': 0.10147101432085037, 'learning_rate': 2.910572722001989e-06, 'epoch': 17.56, 'num_input_tokens_seen': 2535080, 'train_runtime': 2715.6409, 'train_tokens_per_second': 933.511}\n",
      " 90%|████████████████████████████████████▊    | 880/980 [46:19<05:49,  3.50s/it][INFO|2025-05-23 13:03:58] llamafactory.train.callbacks:143 >> {'loss': 0.0039, 'learning_rate': 2.0402e-06, 'epoch': 17.97, 'throughput': 933.77}\n",
      "{'loss': 0.0039, 'grad_norm': 0.10973681509494781, 'learning_rate': 2.0401682832299845e-06, 'epoch': 17.97, 'num_input_tokens_seen': 2595592, 'train_runtime': 2779.6988, 'train_tokens_per_second': 933.767}\n",
      " 92%|█████████████████████████████████████▋   | 900/980 [47:19<03:55,  2.94s/it][INFO|2025-05-23 13:04:58] llamafactory.train.callbacks:143 >> {'loss': 0.0040, 'learning_rate': 1.3187e-06, 'epoch': 18.37, 'throughput': 934.24}\n",
      "{'loss': 0.004, 'grad_norm': 0.2833060622215271, 'learning_rate': 1.318667163553733e-06, 'epoch': 18.37, 'num_input_tokens_seen': 2652960, 'train_runtime': 2839.7119, 'train_tokens_per_second': 934.236}\n",
      " 94%|██████████████████████████████████████▍  | 920/980 [48:23<02:58,  2.98s/it][INFO|2025-05-23 13:06:02] llamafactory.train.callbacks:143 >> {'loss': 0.0031, 'learning_rate': 7.5075e-07, 'epoch': 18.78, 'throughput': 934.36}\n",
      "{'loss': 0.0031, 'grad_norm': 0.22181521356105804, 'learning_rate': 7.507485745971043e-07, 'epoch': 18.78, 'num_input_tokens_seen': 2712832, 'train_runtime': 2903.418, 'train_tokens_per_second': 934.358}\n",
      " 96%|███████████████████████████████████████▎ | 940/980 [49:25<02:02,  3.07s/it][INFO|2025-05-23 13:07:05] llamafactory.train.callbacks:143 >> {'loss': 0.0034, 'learning_rate': 3.4010e-07, 'epoch': 19.19, 'throughput': 934.23}\n",
      "{'loss': 0.0034, 'grad_norm': 0.5331394672393799, 'learning_rate': 3.4009568633777964e-07, 'epoch': 19.19, 'num_input_tokens_seen': 2770864, 'train_runtime': 2965.9436, 'train_tokens_per_second': 934.227}\n",
      " 98%|████████████████████████████████████████▏| 960/980 [50:29<01:04,  3.21s/it][INFO|2025-05-23 13:08:08] llamafactory.train.callbacks:143 >> {'loss': 0.0033, 'learning_rate': 8.9372e-08, 'epoch': 19.60, 'throughput': 934.84}\n",
      "{'loss': 0.0033, 'grad_norm': 0.1422652304172516, 'learning_rate': 8.937174033659956e-08, 'epoch': 19.6, 'num_input_tokens_seen': 2832336, 'train_runtime': 3029.7488, 'train_tokens_per_second': 934.842}\n",
      "100%|█████████████████████████████████████████| 980/980 [51:32<00:00,  3.06s/it][INFO|2025-05-23 13:09:11] llamafactory.train.callbacks:143 >> {'loss': 0.0038, 'learning_rate': 2.0278e-10, 'epoch': 20.00, 'throughput': 933.92}\n",
      "{'loss': 0.0038, 'grad_norm': 0.9058072566986084, 'learning_rate': 2.027775940743881e-10, 'epoch': 20.0, 'num_input_tokens_seen': 2887792, 'train_runtime': 3092.1172, 'train_tokens_per_second': 933.921}\n",
      "100%|█████████████████████████████████████████| 980/980 [51:32<00:00,  3.06s/it][INFO|trainer.py:3993] 2025-05-23 13:09:11,364 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/hr_bot/checkpoint-980\n",
      "[INFO|configuration_utils.py:698] 2025-05-23 13:09:11,971 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json\n",
      "[INFO|configuration_utils.py:770] 2025-05-23 13:09:11,973 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2356] 2025-05-23 13:09:12,338 >> chat template saved in saves/Llama-3.2-3B-Instruct/lora/hr_bot/checkpoint-980/chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2525] 2025-05-23 13:09:12,344 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/hr_bot/checkpoint-980/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2534] 2025-05-23 13:09:12,347 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/hr_bot/checkpoint-980/special_tokens_map.json\n",
      "[INFO|trainer.py:2676] 2025-05-23 13:09:13,309 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 3094.0658, 'train_samples_per_second': 10.013, 'train_steps_per_second': 0.317, 'train_loss': 0.42374031193250294, 'epoch': 20.0, 'num_input_tokens_seen': 2887792}\n",
      "100%|█████████████████████████████████████████| 980/980 [51:34<00:00,  3.16s/it]\n",
      "[INFO|trainer.py:3993] 2025-05-23 13:09:13,320 >> Saving model checkpoint to saves/Llama-3.2-3B-Instruct/lora/hr_bot\n",
      "[INFO|configuration_utils.py:698] 2025-05-23 13:09:13,855 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json\n",
      "[INFO|configuration_utils.py:770] 2025-05-23 13:09:13,856 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2356] 2025-05-23 13:09:14,207 >> chat template saved in saves/Llama-3.2-3B-Instruct/lora/hr_bot/chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2525] 2025-05-23 13:09:14,212 >> tokenizer config file saved in saves/Llama-3.2-3B-Instruct/lora/hr_bot/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2534] 2025-05-23 13:09:14,214 >> Special tokens file saved in saves/Llama-3.2-3B-Instruct/lora/hr_bot/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =       20.0\n",
      "  num_input_tokens_seen    =    2887792\n",
      "  total_flos               = 45681723GF\n",
      "  train_loss               =     0.4237\n",
      "  train_runtime            = 0:51:34.06\n",
      "  train_samples_per_second =     10.013\n",
      "  train_steps_per_second   =      0.317\n",
      "Figure saved at: saves/Llama-3.2-3B-Instruct/lora/hr_bot/training_loss.png\n",
      "[WARNING|2025-05-23 13:09:14] llamafactory.extras.ploting:148 >> No metric eval_loss to plot.\n",
      "[WARNING|2025-05-23 13:09:14] llamafactory.extras.ploting:148 >> No metric eval_accuracy to plot.\n",
      "[INFO|modelcard.py:450] 2025-05-23 13:09:14,719 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-05-26 05:36:52,735 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-05-26 05:36:52,735 >> loading file tokenizer.model from cache at None\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-05-26 05:36:52,735 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-05-26 05:36:52,735 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-05-26 05:36:52,735 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-05-26 05:36:52,736 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|tokenization_utils_base.py:2299] 2025-05-26 05:36:53,170 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:698] 2025-05-26 05:36:54,398 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json\n",
      "[INFO|configuration_utils.py:770] 2025-05-26 05:36:54,529 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-05-26 05:36:54,969 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-05-26 05:36:54,969 >> loading file tokenizer.model from cache at None\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-05-26 05:36:54,969 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-05-26 05:36:54,969 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-05-26 05:36:54,969 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-05-26 05:36:54,969 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|tokenization_utils_base.py:2299] 2025-05-26 05:36:55,282 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|2025-05-26 05:36:55] llamafactory.data.template:143 >> Add pad token: <|eot_id|>\n",
      "[INFO|2025-05-26 05:36:55] llamafactory.data.template:143 >> Add <|eom_id|> to stop words.\n",
      "[INFO|configuration_utils.py:698] 2025-05-26 05:36:55,499 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json\n",
      "[INFO|configuration_utils.py:770] 2025-05-26 05:36:55,500 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|2025-05-26 05:36:55] llamafactory.model.model_utils.kv_cache:143 >> KV cache is enabled for faster generation.\n",
      "[INFO|modeling_utils.py:1149] 2025-05-26 05:37:14,853 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:2239] 2025-05-26 05:37:14,855 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1135] 2025-05-26 05:37:14,859 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ]\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:05<00:00,  2.87s/it]\n",
      "[INFO|modeling_utils.py:5170] 2025-05-26 05:37:21,096 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:5178] 2025-05-26 05:37:21,096 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-3.2-3B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:1090] 2025-05-26 05:37:21,311 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/generation_config.json\n",
      "[INFO|configuration_utils.py:1135] 2025-05-26 05:37:21,311 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "[INFO|2025-05-26 05:37:21] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
      "[INFO|2025-05-26 05:37:25] llamafactory.model.adapter:143 >> Merged 1 adapter(s).\n",
      "[INFO|2025-05-26 05:37:25] llamafactory.model.adapter:143 >> Loaded adapter(s): saves/Llama-3.2-3B-Instruct/lora/hr_bot\n",
      "[INFO|2025-05-26 05:37:25] llamafactory.model.loader:143 >> all params: 3,212,749,824\n",
      "[WARNING|2025-05-26 05:37:25] llamafactory.chat.hf_engine:154 >> There is no current event loop, creating a new one.\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli webui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c73a2d2-035d-49df-9b22-a55908501480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/work/alma/hrChatBot\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ced75913-bcc8-453e-b756-7bcfb1b82e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/work/alma/hrChatBot/LLaMA-Factory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/work/alma/hrChatBot/env/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd LLaMA-Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "757958b4-a8fc-4de1-b1c1-7b04190115f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/work/alma/hrChatBot/LLaMA-Factory\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58ea5ffd-0c6d-450b-b1ae-a2e35b1dac07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamafactory.chat import ChatModel\n",
    "from llamafactory.extras.misc import torch_gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7f53be5-0d1d-4f70-9888-3286e0db85de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:2023] 2025-05-26 13:45:38,285 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-05-26 13:45:38,286 >> loading file tokenizer.model from cache at None\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-05-26 13:45:38,287 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-05-26 13:45:38,288 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-05-26 13:45:38,289 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-05-26 13:45:38,291 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|tokenization_utils_base.py:2299] 2025-05-26 13:45:38,735 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:698] 2025-05-26 13:45:40,415 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json\n",
      "[INFO|configuration_utils.py:770] 2025-05-26 13:45:40,483 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-05-26 13:45:40,908 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-05-26 13:45:40,908 >> loading file tokenizer.model from cache at None\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-05-26 13:45:40,909 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-05-26 13:45:40,910 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-05-26 13:45:40,910 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2023] 2025-05-26 13:45:40,911 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|tokenization_utils_base.py:2299] 2025-05-26 13:45:41,225 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|2025-05-26 13:45:41] llamafactory.data.template:143 >> Add pad token: <|eot_id|>\n",
      "[INFO|2025-05-26 13:45:41] llamafactory.data.template:143 >> Add <|eom_id|> to stop words.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:698] 2025-05-26 13:45:41,503 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json\n",
      "[INFO|configuration_utils.py:770] 2025-05-26 13:45:41,505 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|2025-05-26 13:45:41] llamafactory.model.model_utils.kv_cache:143 >> KV cache is enabled for faster generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:1149] 2025-05-26 13:45:50,489 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:2239] 2025-05-26 13:45:50,491 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1135] 2025-05-26 13:45:50,497 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ]\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19b7316f1d1e430d8c34d04d8b16e133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:5170] 2025-05-26 13:45:53,594 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:5178] 2025-05-26 13:45:53,595 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-3.2-3B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:1090] 2025-05-26 13:45:53,819 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/generation_config.json\n",
      "[INFO|configuration_utils.py:1135] 2025-05-26 13:45:53,820 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|2025-05-26 13:45:54] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
      "[INFO|2025-05-26 13:45:57] llamafactory.model.adapter:143 >> Merged 1 adapter(s).\n",
      "[INFO|2025-05-26 13:45:57] llamafactory.model.adapter:143 >> Loaded adapter(s): ./saves/Llama-3.2-3B-Instruct/lora/hr_bot\n",
      "[INFO|2025-05-26 13:45:57] llamafactory.model.loader:143 >> all params: 3,212,749,824\n"
     ]
    }
   ],
   "source": [
    "args = dict(\n",
    "  model_name_or_path=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "  adapter_name_or_path=\"./saves/Llama-3.2-3B-Instruct/lora/hr_bot\",                        # load the saved LoRA adapters\n",
    "  template=\"llama3\",                                         # same to the one in training\n",
    "  finetuning_type=\"lora\",                                    # same to the one in training\n",
    ")\n",
    "chat_model = ChatModel(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d93be49-da31-43f0-83c1-e64c53dc5399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the CLI application. Type `exit` to exit.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "User:  hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Hello! I am Dhruv, an AI assistant at Wrk Talk developed by Pramod Kolekar. How can I assist you today?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "User:  what is wrk talk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Wrk Talk is a secure internal communication platform developed by WrkTalk DigiSec AI Private Limited, a Mumbai-based company founded on April 15, 2024. Designed for organizations that prioritize privacy, Wrk Talk offers features such as instant messaging, conference calls, secure file storage, and more, ensuring that all communications remain within the organization. The platform emphasizes data security, with no data collection or sharing with third parties, and is available on both Android and iOS devices.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "User:  what is location of company ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Mumbai, India\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "User:  what about working hours of company?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: 9:30 AM to 6:30 PM, Monday to Friday.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "User:   How long is the probation period for a new employee?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: 6 months\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "User:   How many days of paid annual leaves are available\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: 18 days.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "User:  What expenses are covered by the policy?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Travel expenses, hotel accommodation, meals during business travel, client entertainment within approved limits, stationery and office supplies, internet and mobile charges when used for business purposes.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "User:  exit\n"
     ]
    }
   ],
   "source": [
    "print(\"Welcome to the CLI application. Type `exit` to exit.\")\n",
    "\n",
    "while True:\n",
    "  query = input(\"\\nUser: \")\n",
    "  if query.strip() == \"exit\":\n",
    "    break\n",
    "\n",
    "  # Stateless behavior: each time, create a fresh message list\n",
    "  messages = [{\"role\": \"user\", \"content\": query}]\n",
    "\n",
    "  print(\"Assistant: \", end=\"\", flush=True)\n",
    "  response = \"\"\n",
    "  for new_text in chat_model.stream_chat(messages):\n",
    "    print(new_text, end=\"\", flush=True)\n",
    "    response += new_text\n",
    "  print()\n",
    "\n",
    "torch_gc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39824ceb-3d65-4a4b-9203-bb9e4d43fef2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
